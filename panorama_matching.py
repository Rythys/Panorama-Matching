# -*- coding: utf-8 -*-
"""panorama_matching.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tTzSyWFBGVePd1VVnUGJ6foXAfDr4MKs

## Локальные дескрипторы изображений и построение панорам
"""

import json
import os
import cv2
import random
import glob
import torch

import numpy as np

from skimage import io # for io.imread
from IPython.display import clear_output
from matplotlib import pyplot as plt
from matplotlib import colors # ploting

def imshow(images, titles, nrows = 0, ncols=0, figsize = (15,20)):
    """Plot a multiple images with titles.

    Parameters
    ----------
    images : image list
    titles : title list
    ncols : number of columns of subplots wanted in the display
    nrows : number of rows of subplots wanted in the figure
    """

    if ncols == 0 and nrows == 0:
      ncols = len(images)
      nrows = 1
    if ncols == 0:
      ncols = len(images) // nrows
    if nrows == 0:
      nrows = len(images) // ncols

    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows, squeeze=False, figsize = figsize)
    for i, image in enumerate(images):
        axeslist.ravel()[i].imshow(image, cmap=plt.gray(), vmin=0, vmax=255)
        axeslist.ravel()[i].set_title(titles[i])
        axeslist.ravel()[i].set_axis_off()
    plt.tight_layout() # optional

if not os.path.exists('./images'):
    os.mkdir('./images')
else:
    print('folder `images` is present, passing')

!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example1_1.jpeg
!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example1_2.jpeg
!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example2_1.jpeg
!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example2_2.jpeg
!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example3_1.jpeg
!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example3_2.jpeg
!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example4_1.jpeg
!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example4_2.jpeg
!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example5_1.jpeg
!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example5_2.jpeg
!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example6_1.jpeg
!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/images/example6_2.jpeg
!mv ./example*.jpeg images

!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/25s_ml_trainings_3/homeworks/hw03_panorama/keypoints_sift.json

# Create a dictionary to store images with the same prefix
image_dict = {}

# Read all images and group them by prefix
for filename in sorted(glob.glob('./images/*.jpeg')):
    name = os.path.basename(filename)
    prefix = name.split('_')[0]  # Get prefix before first underscore

    # Load the image
    img = io.imread(filename)
    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Add to dictionary
    if prefix in image_dict:
        image_dict[prefix].append((img))
    else:
        image_dict[prefix] = [(img)]

image1, image2 = image_dict['example1']
imshow( [image1, image2], ['Left', 'Right'])

"""## Шаг 1: Создание панорамы вручную
Подбор сдвига по координатам X и Y (`tx` и `ty` соответственно) для второго изобаражения относительно первого для задания оптимальной трансляции (смещения) для совмещения изображений вручную. Изображения имеют размеры, приведенные ниже:
"""

print(f"first image shape: {image1.shape}, second image shape: {image2.shape}")

X_SHIFT = 100
Y_SHIFT = 100 # so you can have negative tx and ty
tx = 350
ty = -10

assert tx + X_SHIFT >= 0
assert ty + Y_SHIFT >= 0

size = (image1.shape[0] + image2.shape[0], image1.shape[1] + image2.shape[1], 3)
image_trans = np.uint8(np.zeros(size))

# put image 1 on resulting image
image_trans[Y_SHIFT:Y_SHIFT+image1.shape[0], X_SHIFT:X_SHIFT+image1.shape[1], :] = image1

# put image 2 on resulting image
image_trans[Y_SHIFT+ty:Y_SHIFT+ty+image2.shape[0], X_SHIFT+tx:X_SHIFT+tx+image2.shape[1], :] = image2

# #add vertical line where two images are joined, use red color
image_trans[:, X_SHIFT+tx, :] = [255, 0, 0]
image_trans[Y_SHIFT+ty, :, :] = [0, 255, 0]

imshow( [image_trans], ['Translation-based panorama'])

"""## Шаг 2: Обнаружение ключевых точек

Использование **SIFT** из OpenCV (`cv2.SIFT_create`) для обнаружения ключевых точек и вычисления их дескрипторов на обоих изображениях.  
Для этого реализуем функцию `extract_key_points`:
"""

def extract_key_points(img1, img2):
    # readimage1 = cv2.imread(img1)
    # readimage2 = cv2.imread(img2)
    sift = cv2.SIFT_create()
    kpts1, desc1 = sift.detectAndCompute(img1, None)
    kpts2, desc2 = sift.detectAndCompute(img2, None)
    return kpts1, desc1, kpts2, desc2

kp1, des1, kp2, des2 = extract_key_points(image1, image2)


print("Coordinates of the first keypoint of image1: ", kp1[0].pt)
print("Descriptor of the first keypoint of image1:\n", des1[0])

"""## Шаг 2: Сопоставление ключевых точек  
Далее необходимо сопоставить признаки между изображениями. Существует множество стратегий сопоставления, давайте используем самый простой подход, реализованный в [BFMatcher](https://opencv24-python-tutorials.readthedocs.io/en/stable/py_tutorials/py_feature2d/py_matcher/py_matcher.html) (от Brute Force).

Оценить качество сопоставления ключевых точек можно на основе аттрибута `distance`. Полезно отсортировать соответствия по возрастанию расстояния, чтобы первые элементы списка были наиболее релевантными.
"""

def match_key_points_cv(des1, des2):
    bf =  cv2.BFMatcher(crossCheck=True)
    matches = bf.match(des1, des2)

    sorted_matches = sorted(matches, key = lambda x:x.distance)
    return sorted_matches

def showMatches(img1, kp1, img2, kp2, matches, name):
    img = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
    imshow([img],[name])

matches_cv = match_key_points_cv(des1, des2)

print(len(matches_cv))
showMatches(image1,kp1,image2,kp2,matches_cv,"all matches")
showMatches(image1,kp1,image2,kp2,matches_cv[:50],"best 50 matches")
print(len(matches_cv))

"""Реализация аналога `BFMatcher`, используя чистый `numpy`. Он основан на подсчете разницы между дескрипторами изображений и вычисляется следующим образом:

1. Подсчет матрицы попарных расстояний между дескрипторами обоих изображений.
2. Для каждого дескриптора изображения 1 находится наиболее похожий (ближайший) дескриптор изображения 2.
2. Для каждого дескриптора изображения 2 находится наиболее похожий (ближайший) дескриптор изображения 1.
4. Те пары дескрипторов, которые совпали (т.е. дескрипторы ближайшие друг для друга) считаются парами.

"""

class DummyMatch:
    def __init__(self, queryIdx, trainIdx, distance):
        self.queryIdx = queryIdx  # index in des1
        self.trainIdx = trainIdx  # index in des2
        self.distance = distance

import numpy as np
def match_key_points_numpy(des1: np.ndarray, des2: np.ndarray) -> list:
    """
    Match descriptors using brute-force matching with cross-check.

    Args:
        des1 (np.ndarray): Descriptors from image 1, shape (N1, D)
        des2 (np.ndarray): Descriptors from image 2, shape (N2, D)

    Returns:
        List[DummyMatch]: Sorted list of mutual best matches.
    """

    matrix1 = np.zeros((des1.shape[0], des2.shape[0]))
    match1_2 = []
    match2_1 = []
    matches_dist = []
    matches = []
    for i in range(des1.shape[0]):
      for j in range(des2.shape[0]):
        matrix1[i,j] = np.sqrt(np.sum((des1[i] - des2[j])**2))
      match1_2.append([i, np.argmin(matrix1[i])])
      matches_dist.append(np.min(matrix1[i]))
    matrix2 = np.transpose(matrix1)
    for j in range(des2.shape[0]):
      match2_1.append([np.argmin(matrix2[j]), j])
    for i in range(len(match1_2)):
      for j in range(len(match2_1)):
        if (match1_2[i] == match2_1[j]):
          idx1, idx2 = match1_2[i]
          dist = matches_dist[i]
          matches.append(DummyMatch(idx1, idx2, dist))

    sorted_matches = sorted(matches, key = lambda x:x.distance)
    return sorted_matches

"""Проверка:"""

def test_numpy_bf_matcher_equivalence(des1, des2):
    # OpenCV BFMatcher
    cv_matches = match_key_points_cv(des1, des2)

    # Our matcher
    np_matches = match_key_points_numpy(des1, des2)

    # Compare match indices and distances
    assert len(cv_matches) == len(np_matches), f"Match count mismatch: {len(cv_matches)} vs {len(np_matches)}"

    for idx, (m_cv, m_np) in enumerate(zip(cv_matches, np_matches)):
        assert m_cv.queryIdx == m_np.queryIdx
        assert m_cv.trainIdx == m_np.trainIdx
        assert abs(m_cv.distance - m_np.distance) < 1e-4, f"Distance mismatch on {idx}th match: {m_cv.distance:.4f} vs {m_np.distance:.4f}"

    print("Numpy implementation matches OpenCV BFMatcher output!")

test_numpy_bf_matcher_equivalence(des1, des2)

"""Интересная реализация поворота квадрата на 90 градусов"""

import numpy as np

def warp_image(img, M, output_shape):
    """
    Warp an image using a transformation matrix M with numpy only.

    Args:
        img (np.ndarray): Input image (H, W) or (H, W, C).
        M (np.ndarray): 3x3 transformation matrix.
        output_shape (tuple): Output image shape (H_out, W_out).

    Returns:
        np.ndarray: Warped image.
    """
    H_out, W_out = output_shape
    if img.ndim == 2:
        img = img[..., None]  # Add channel axis for simplicity

    H, W, C = img.shape
    warped = np.zeros((H_out, W_out, C), dtype=img.dtype)

    # Inverse transform (destination -> source)
    M_inv = np.linalg.inv(M)

    # Generate (x, y) grid for output image
    ys, xs = np.indices((H_out, W_out))
    ones = np.ones_like(xs)
    coords = np.stack([xs, ys, ones], axis=-1).reshape(-1, 3).T  # shape (3, N)

    # Apply inverse transform
    src_coords = M_inv @ coords
    src_xs = src_coords[0, :]
    src_ys = src_coords[1, :]

    # Nearest neighbor sampling
    src_xs = np.round(src_xs).astype(int)
    src_ys = np.round(src_ys).astype(int)

    # Mask for valid coordinates
    valid = (0 <= src_xs) & (src_xs < W) & (0 <= src_ys) & (src_ys < H)

    dst_xs = coords[0, valid].astype(int)
    dst_ys = coords[1, valid].astype(int)

    warped[dst_ys, dst_xs] = img[src_ys[valid], src_xs[valid]]

    return warped if C > 1 else warped[..., 0]

import matplotlib.pyplot as plt

# Dummy image
img = np.zeros((100, 100), dtype=np.uint8)
img[30:70, 30:70] = 255  # White square

# Affine transform: rotate 45 degrees around center
theta = np.deg2rad(45)
cos_t, sin_t = np.cos(theta), np.sin(theta)
center = np.array([50, 50])

# Translation to origin -> rotate -> translate back
T1 = np.array([[1, 0, -center[0]],
               [0, 1, -center[1]],
               [0, 0, 1]])
R = np.array([[cos_t, -sin_t, 0],
              [sin_t, cos_t, 0],
              [0, 0, 1]])
T2 = np.array([[1, 0, center[0]],
               [0, 1, center[1]],
               [0, 0, 1]])
M = T2 @ R @ T1

# Warp
warped = warp_image(img, M, (100, 100))

# Show
plt.subplot(1, 2, 1)
plt.imshow(img, cmap='gray')
plt.title('Original')
plt.subplot(1, 2, 2)
plt.imshow(warped, cmap='gray')
plt.title('Warped')
plt.show()

"""## Шаг 4: Оценка матрицы гомографии с использованием DLT и RANSAC

Функция cv2.findHomography оценивает гомографию, которая преобразует исходные точки в целевые. При использовании метода RANSAC она устойчива к выбросам. Ниже доступна реализация функции с использованием RANSAC, а также с использованием более простого метода DLT.

Далее идет реализацияь DLT на чистом `numpy`.
"""

def findHomography_dlt_opencv(matches, keypoint1, keypoint2, mode='DLT'):

    src_pts = np.float32([keypoint1[m.queryIdx].pt for m in matches]).reshape(-1,1,2)
    dst_pts = np.float32([keypoint2[m.trainIdx].pt for m in matches]).reshape(-1,1,2)

    if mode == 'DLT':
        mode = 0
    elif mode == 'RANSAC':
        mode = cv2.RANSAC
    H, mask = cv2.findHomography(src_pts, dst_pts, mode)
    mask = mask.ravel().tolist()

    inliers = []
    for i in range(len(mask)):
      if mask[i] == 1:
        inliers.append(matches[i])

    return H, inliers


H_for_panorama, inliers = findHomography_dlt_opencv(matches_cv, kp1, kp2, 'RANSAC')
showMatches(image1,kp1,image2,kp2,inliers,"inliers only, RANSAC")


H, inliers = findHomography_dlt_opencv(matches_cv, kp1, kp2, 'DLT')
showMatches(image1,kp1,image2,kp2,inliers,"DLT, all matches")

H, inliers = findHomography_dlt_opencv(matches_cv[:50], kp1, kp2, 'DLT')
showMatches(image1,kp1,image2,kp2,inliers,"DLT, top 50 matches")

"""Пример реализации DLT с развернутым описанием и разбором можно найти [здесь](https://medium.com/@insight-in-plain-sight/estimating-the-homography-matrix-with-the-direct-linear-transform-dlt-ec6bbb82ee2b)."""

def dlt_homography_normalized(pts1: np.ndarray, pts2: np.ndarray) -> np.ndarray:
    """
    Computes homography matrix using normalized Direct Linear Transform (DLT).

    Args:
        pts1 (np.ndarray): Source points (N, 2)
        pts2 (np.ndarray): Destination points (N, 2)

    Returns:
        np.ndarray: Homography matrix (3x3)
    """

    def construct_A_partial(point_source, point_target):
        x, y, z = point_source[0][0], point_source[0][1], 1
        x_t, y_t, z_t = point_target[0][0], point_target[0][1], 1

        A_partial = np.array([
            [0, 0, 0, -z_t*x, -z_t*y, -z_t*z, y_t*x, y_t*y, y_t*z],
            [z_t*x, z_t*y, z_t*z, 0, 0, 0, -x_t*x, -x_t*y, -x_t*z]
        ])
        return A_partial

    def construct_A(points_source, points_target):
        num_points = points_source.shape[0]

        matrices = []
        for i in range(num_points):
            partial_A = construct_A_partial(points_source[i], points_target[i])
            matrices.append(partial_A)
        return np.concatenate(matrices, axis=0)
    def find_homography(points_source, points_target):
        A  = construct_A(points_source, points_target)
        u, s, vh = np.linalg.svd(A, full_matrices=True)

        homography = vh[-1].reshape((3,3))
        return homography/homography[2,2]
    H = find_homography(pts1,pts2)

    return H


def findHomography_dlt_numpy(matches, keypoint1, keypoint2):
    src_pts = np.float32([keypoint1[m.queryIdx].pt for m in matches]).reshape(-1,1,2)
    dst_pts = np.float32([keypoint2[m.trainIdx].pt for m in matches]).reshape(-1,1,2)
    return dlt_homography_normalized(src_pts, dst_pts), None

"""Т.к. DLT неустойчив к выбросам (и вообще достаточно прост), будем использовать `NUM_BEST_MATCHES` лучших совпадений"""

NUM_BEST_MATCHES = 50

kp1, des1, kp2, des2 = extract_key_points(*image_dict['example2'])
matches_cv = match_key_points_cv(des1, des2)

H_numpy, _ = findHomography_dlt_numpy(matches_cv[:NUM_BEST_MATCHES], kp1, kp2)
H, _ = findHomography_dlt_opencv(matches_cv[:NUM_BEST_MATCHES], kp1, kp2, mode='DLT')
assert np.allclose(H_numpy, H, atol=2e-1), f"Homography matrices are too different!\nH_numpy:\n{H_numpy}\nH from opencv:\n{H}"

"""## Шаг 5. Построение панорамы
Используя матрицу гомографии `H` можно совместить две фотографии.
"""

def panorama(img1, img2, H, size):
    img = np.uint8(np.zeros(size))
    img = cv2.warpPerspective(src=img1, dst=img, M=np.eye(3), dsize=(size[1], size[0]))
    img = cv2.warpPerspective(src=img2, dst=img, M=H, dsize=(size[1], size[0]), flags=cv2.WARP_INVERSE_MAP, borderMode=cv2.BORDER_TRANSPARENT)

    return img

size = (1280, 960*2, 3)
imshow([panorama(image1, image2, H_for_panorama, size)],["Panorama"])

"""## Шаг 6: Функция для построения панорам
Осталось лишь собрать воедино наработки (кроме ручного DLT) и получить функцию, которая на входе имеет два изображения, а на выходе возвращает совмещенное изображение и матрицу `H`.
Благодаря использованию SIFT угол съемки может варьироваться.

**Примечание: т.к. вариантов построения данной функции много, будем использовать следующую последовательность:**
1. Обнаружение ключевых точек с помощью фунции `extract_key_points` с SIFT под капотом.
2. Для сопоставления ключевых точек используется функцию `match_key_points_cv`.
3. Для определения матрицы гомографии используется `findHomography_dlt_opencv` с методом `RANSAC`.
"""

def panorama_pipeline(img1, img2, size):

    sift = cv2.SIFT_create()
    kp1, des1 = sift.detectAndCompute(img1, None)
    kp2, des2 = sift.detectAndCompute(img2, None)
    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)
    matches = bf.match(des1, des2)
    matches = sorted(matches, key=lambda x: x.distance)
    matches = matches[:50]

    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)

    H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC)
    panorama = np.uint8(np.zeros(size))
    cv2.warpPerspective(src=img1, dst=panorama, M=np.eye(3), dsize=(size[1], size[0]))
    cv2.warpPerspective(src=img2, dst=panorama, M=H, dsize=(size[1], size[0]), flags=cv2.WARP_INVERSE_MAP, borderMode=cv2.BORDER_TRANSPARENT)

    return panorama, H

size = (1280, 960*2, 3)
h_dict = {}

for filename, (img1, img2) in image_dict.items():
    final_image, H = panorama_pipeline(img1, img2, size)
    h_dict[filename] = H.tolist()
    imshow([final_image],[filename])
with open('h_submission_dict.json', 'w') as iofile:
    json.dump(h_dict, iofile)

"""Конец"""